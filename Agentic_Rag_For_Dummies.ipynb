{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovanniPasq/agentic-rag-for-dummies/blob/main/Agentic_Rag_For_Dummies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "321dbfd3",
      "metadata": {
        "id": "321dbfd3"
      },
      "outputs": [],
      "source": [
        "#run this cell only in colab, otherwise create a venv and install requirements.txt available in the project folder\n",
        "!pip install --quiet --upgrade langgraph\n",
        "!pip install -qU \"langchain[google-genai]\"\n",
        "!pip install -qU langchain langchain-community langchain-qdrant langchain-huggingface qdrant-client fastembed flashrank langchain-core\n",
        "!pip install --upgrade gradio\n",
        "\n",
        "# Optional: if you want to use Ollama with local models\n",
        "!pip install -qU langchain-ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9782958",
      "metadata": {
        "id": "c9782958",
        "outputId": "af44c2b9-6e24-4afa-aa11-bb355b28d90b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Configuration loaded\n",
            "  - Docs directory: docs\n",
            "  - Parent store: parent_store\n",
            "  - Collection name: document_child_chunks\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# BLOCK 1: Configuration and Environment Setup\n",
        "# ============================================================\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Configuration\n",
        "DOCS_DIR = \"docs\"  # Directory containing your .md files\n",
        "PARENT_STORE_PATH = \"parent_store\"  # Directory for parent chunk JSON files\n",
        "CHILD_COLLECTION = \"document_child_chunks\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(DOCS_DIR, exist_ok=True)\n",
        "os.makedirs(PARENT_STORE_PATH, exist_ok=True)\n",
        "\n",
        "print(\"âœ“ Configuration loaded\")\n",
        "print(f\"  - Docs directory: {DOCS_DIR}\")\n",
        "print(f\"  - Parent store: {PARENT_STORE_PATH}\")\n",
        "print(f\"  - Collection name: {CHILD_COLLECTION}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e8989b7",
      "metadata": {
        "id": "7e8989b7"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOCK 2: LLM Initialization\n",
        "# ============================================================\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "# Initialize LLM\n",
        "llm = ChatOllama(model=\"qwen3:4b-instruct-2507-q4_K_M\", temperature=0.1)\n",
        "\n",
        "# Alternative: Google Gemini\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"your-api-key-here\"\n",
        "# llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", temperature=0)\n",
        "\n",
        "print(\"âœ“ LLM initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f982c53",
      "metadata": {
        "id": "9f982c53",
        "outputId": "67a2f7ce-9d1e-44c9-fd2e-e760c3d20c42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Embeddings initialized\n",
            "  - Dense model: sentence-transformers/all-mpnet-base-v2\n",
            "  - Sparse model: Qdrant/bm25\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# BLOCK 3: Embeddings Setup\n",
        "# ============================================================\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_qdrant.fastembed_sparse import FastEmbedSparse\n",
        "\n",
        "# Dense embeddings for semantic understanding\n",
        "dense_embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        ")\n",
        "\n",
        "# Sparse embeddings for keyword matching\n",
        "sparse_embeddings = FastEmbedSparse(\n",
        "    model_name=\"Qdrant/bm25\"\n",
        ")\n",
        "\n",
        "print(\"âœ“ Embeddings initialized\")\n",
        "print(f\"  - Dense model: sentence-transformers/all-mpnet-base-v2\")\n",
        "print(f\"  - Sparse model: Qdrant/bm25\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "11efc63b",
      "metadata": {
        "id": "11efc63b",
        "outputId": "fdbede67-1281-416a-b20e-56139568c111"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Created collection: document_child_chunks\n",
            "âœ“ Vector store initialized\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# BLOCK 4: Vector Database Setup\n",
        "# ============================================================\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models as qmodels\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain_qdrant.qdrant import RetrievalMode\n",
        "\n",
        "# Initialize Qdrant client (local file-based storage)\n",
        "client = QdrantClient(path=\"qdrant_db\")\n",
        "\n",
        "# Get embedding dimension\n",
        "embedding_dimension = len(dense_embeddings.embed_query(\"test\"))\n",
        "\n",
        "def ensure_collection(collection_name):\n",
        "    \"\"\"Create Qdrant collection if it doesn't exist\"\"\"\n",
        "    if not client.collection_exists(collection_name):\n",
        "        client.create_collection(\n",
        "            collection_name=collection_name,\n",
        "            vectors_config=qmodels.VectorParams(\n",
        "                size=embedding_dimension,\n",
        "                distance=qmodels.Distance.COSINE\n",
        "            ),\n",
        "            sparse_vectors_config={\n",
        "                \"sparse\": qmodels.SparseVectorParams()\n",
        "            },\n",
        "        )\n",
        "        print(f\"âœ“ Created collection: {collection_name}\")\n",
        "    else:\n",
        "        print(f\"âœ“ Collection already exists: {collection_name}\")\n",
        "\n",
        "# Create collection\n",
        "ensure_collection(CHILD_COLLECTION)\n",
        "\n",
        "# Initialize vector store for child chunks\n",
        "child_vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=CHILD_COLLECTION,\n",
        "    embedding=dense_embeddings,\n",
        "    sparse_embedding=sparse_embeddings,\n",
        "    retrieval_mode=RetrievalMode.HYBRID,\n",
        "    sparse_vector_name=\"sparse\"\n",
        ")\n",
        "\n",
        "print(\"âœ“ Vector store initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e103d352",
      "metadata": {
        "id": "e103d352",
        "outputId": "c6aa9ed1-f9a2-403c-9ee1-a4521752b9ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Starting Hierarchical Indexing\n",
            "==================================================\n",
            "\n",
            "ðŸ“„ Processing: blockchain.md\n",
            "ðŸ“„ Processing: fortinet.md\n",
            "ðŸ“„ Processing: javascript_tutorial.md\n",
            "ðŸ“„ Processing: microservices.md\n",
            "\n",
            "ðŸ” Indexing 2447 child chunks into Qdrant...\n",
            "âœ“ Child chunks indexed successfully\n",
            "ðŸ’¾ Saving 386 parent chunks to JSON...\n",
            "âœ“ Parent chunks saved successfully\n",
            "\n",
            "==================================================\n",
            "âœ“ Indexing Complete!\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# BLOCK 5: Document Indexing\n",
        "# ============================================================\n",
        "import glob\n",
        "import json\n",
        "from langchain_text_splitters import (\n",
        "    MarkdownHeaderTextSplitter,\n",
        "    RecursiveCharacterTextSplitter\n",
        ")\n",
        "\n",
        "def index_documents():\n",
        "    \"\"\"Index documents using hierarchical Parent/Child strategy\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Starting Hierarchical Indexing\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    # Parent splitter: by Markdown headers\n",
        "    headers_to_split_on = [\n",
        "        (\"#\", \"H1\"),\n",
        "        (\"##\", \"H2\"),\n",
        "        (\"###\", \"H3\")\n",
        "    ]\n",
        "    parent_splitter = MarkdownHeaderTextSplitter(\n",
        "        headers_to_split_on=headers_to_split_on,\n",
        "        strip_headers=False\n",
        "    )\n",
        "\n",
        "    # Child splitter: by character count\n",
        "    child_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=100\n",
        "    )\n",
        "\n",
        "    all_child_chunks = []\n",
        "    all_parent_pairs = []\n",
        "\n",
        "    # Check if docs directory has files\n",
        "    md_files = sorted(glob.glob(os.path.join(DOCS_DIR, \"*.md\")))\n",
        "    if not md_files:\n",
        "        print(f\"âš ï¸  No .md files found in {DOCS_DIR}/\")\n",
        "        print(\"Please add your Markdown documents to continue.\")\n",
        "        return\n",
        "\n",
        "    # Process each document\n",
        "    for doc_path_str in md_files:\n",
        "        doc_path = Path(doc_path_str)\n",
        "        print(f\"ðŸ“„ Processing: {doc_path.name}\")\n",
        "\n",
        "        try:\n",
        "            with open(doc_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                md_text = f.read()\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error reading {doc_path.name}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Split into parent chunks\n",
        "        parent_chunks = parent_splitter.split_text(md_text)\n",
        "\n",
        "        for i, p_chunk in enumerate(parent_chunks):\n",
        "            # Add metadata\n",
        "            p_chunk.metadata[\"source\"] = str(doc_path)\n",
        "            parent_id = f\"{doc_path.stem}_parent_{i}\"\n",
        "            p_chunk.metadata[\"parent_id\"] = parent_id\n",
        "\n",
        "            # Store parent reference\n",
        "            all_parent_pairs.append((parent_id, p_chunk))\n",
        "\n",
        "            # Split into child chunks\n",
        "            child_chunks = child_splitter.split_documents([p_chunk])\n",
        "            all_child_chunks.extend(child_chunks)\n",
        "\n",
        "    # Save child chunks to Qdrant\n",
        "    if all_child_chunks:\n",
        "        print(f\"\\nðŸ” Indexing {len(all_child_chunks)} child chunks into Qdrant...\")\n",
        "        try:\n",
        "            child_vector_store.add_documents(all_child_chunks)\n",
        "            print(\"âœ“ Child chunks indexed successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Error indexing child chunks: {e}\")\n",
        "            return\n",
        "    else:\n",
        "        print(\"âš ï¸  No child chunks to index\")\n",
        "        return\n",
        "\n",
        "    # Save parent chunks to JSON files\n",
        "    if all_parent_pairs:\n",
        "        print(f\"ðŸ’¾ Saving {len(all_parent_pairs)} parent chunks to JSON...\")\n",
        "\n",
        "        # Clear existing parent files\n",
        "        for item in os.listdir(PARENT_STORE_PATH):\n",
        "            os.remove(os.path.join(PARENT_STORE_PATH, item))\n",
        "\n",
        "        # Save each parent chunk\n",
        "        for parent_id, doc in all_parent_pairs:\n",
        "            doc_dict = {\n",
        "                \"page_content\": doc.page_content,\n",
        "                \"metadata\": doc.metadata\n",
        "            }\n",
        "            file_path = os.path.join(PARENT_STORE_PATH, f\"{parent_id}.json\")\n",
        "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(doc_dict, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(\"âœ“ Parent chunks saved successfully\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"âœ“ Indexing Complete!\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# Run indexing\n",
        "index_documents()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8eac029",
      "metadata": {
        "id": "b8eac029",
        "outputId": "b375c7ba-c96f-4723-c1cf-b9d6051293a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Tools defined and bound to LLM\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# BLOCK 6: Tool Definitions\n",
        "# ============================================================\n",
        "import json\n",
        "from typing import List\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool\n",
        "def search_child_chunks(query: str, k: int = 5) -> List[dict]:\n",
        "    \"\"\"\n",
        "    Search for the top K most relevant child chunks.\n",
        "\n",
        "    Args:\n",
        "        query: Search query string\n",
        "        k: Number of results to return\n",
        "\n",
        "    Returns:\n",
        "        List of dicts with content, parent_id, and source\n",
        "    \"\"\"\n",
        "    try:\n",
        "        results = child_vector_store.similarity_search(query, k=k)\n",
        "        return [\n",
        "            {\n",
        "                \"content\": doc.page_content,\n",
        "                \"parent_id\": doc.metadata.get(\"parent_id\", \"\"),\n",
        "                \"source\": doc.metadata.get(\"source\", \"\")\n",
        "            }\n",
        "            for doc in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching child chunks: {e}\")\n",
        "        return []\n",
        "\n",
        "@tool\n",
        "def retrieve_parent_chunks(parent_ids: List[str]) -> List[dict]:\n",
        "    \"\"\"\n",
        "    Retrieve full parent chunks by their IDs.\n",
        "\n",
        "    Args:\n",
        "        parent_ids: List of parent chunk IDs to retrieve\n",
        "\n",
        "    Returns:\n",
        "        List of dicts with content, parent_id, and metadata\n",
        "    \"\"\"\n",
        "    unique_ids = sorted(list(set(parent_ids)))\n",
        "    results = []\n",
        "\n",
        "    for parent_id in unique_ids:\n",
        "        file_path = os.path.join(PARENT_STORE_PATH, parent_id if parent_id.lower().endswith(\".json\") else f\"{parent_id}.json\")\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    doc_dict = json.load(f)\n",
        "                    results.append({\n",
        "                        \"content\": doc_dict[\"page_content\"],\n",
        "                        \"parent_id\": parent_id,\n",
        "                        \"metadata\": doc_dict[\"metadata\"]\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading parent chunk {parent_id}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Bind tools to LLM\n",
        "llm_with_tools = llm.bind_tools([search_child_chunks, retrieve_parent_chunks])\n",
        "\n",
        "print(\"âœ“ Tools defined and bound to LLM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b12809c6",
      "metadata": {
        "id": "b12809c6",
        "outputId": "98beec9d-272b-406a-9da0-246fe976a21c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Agent system prompt configured\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# BLOCK 7: Agent System Prompt\n",
        "# ============================================================\n",
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "AGENT_SYSTEM_PROMPT = \"\"\"\n",
        "You are an intelligent assistant that MUST use the available tools to answer questions.\n",
        "\n",
        "**MANDATORY WORKFLOW - Follow these steps for EVERY question:**\n",
        "\n",
        "1. **ALWAYS start by calling `search_child_chunks`** with the user's query\n",
        "   - Choose appropriate K value (default: 5)\n",
        "\n",
        "2. **Read the retrieved chunks** and check if they answer the question\n",
        "\n",
        "3. **If chunks are incomplete**, call `retrieve_parent_chunks` with parent_id values\n",
        "\n",
        "4. **Answer using ONLY the retrieved information**\n",
        "   - Cite source files from metadata\n",
        "\n",
        "5. **If no relevant information found after searching**, try rephrasing the query once more\n",
        "\n",
        "**CRITICAL**: You MUST call tools before answering. Never respond without first using `search_child_chunks`.\n",
        "\"\"\"\n",
        "\n",
        "agent_system_message = SystemMessage(content=AGENT_SYSTEM_PROMPT)\n",
        "\n",
        "print(\"âœ“ Agent system prompt configured\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4c066b1",
      "metadata": {
        "id": "c4c066b1",
        "outputId": "69fabe28-6da9-4710-aaeb-08b931005f37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ State models defined\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# BLOCK 8: State Definitions\n",
        "# ============================================================\n",
        "from langgraph.graph import MessagesState\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "\n",
        "class State(MessagesState):\n",
        "    questionIsClear: bool\n",
        "    conversation_summary: str = \"\"\n",
        "\n",
        "class QueryAnalysis(BaseModel):\n",
        "    is_clear: bool = Field(\n",
        "        description=\"Indicates if the user's question is clear and answerable.\"\n",
        "    )\n",
        "    questions: List[str] = Field(\n",
        "        description=\"List of rewritten, self-contained questions.\"\n",
        "    )\n",
        "    clarification_needed: str = Field(\n",
        "        description=\"Explanation if the question is unclear.\"\n",
        "    )\n",
        "\n",
        "print(\"âœ“ State models defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "948ca8ee",
      "metadata": {
        "id": "948ca8ee",
        "outputId": "35f187fa-b7d5-4d93-9f9a-0e219a52e788"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ Graph node functions defined\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# BLOCK 9: Graph Node Functions\n",
        "# ============================================================\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, RemoveMessage\n",
        "from typing import Literal\n",
        "\n",
        "def analyze_chat_and_summarize(state: State):\n",
        "    \"\"\"\n",
        "    Analyzes chat history and summarizes key points for context.\n",
        "    \"\"\"\n",
        "    if len(state[\"messages\"]) < 4:  # Need some history to summarize\n",
        "        return {\"conversation_summary\": \"\"}\n",
        "\n",
        "    # Extract relevant messages (excluding current query and system messages)\n",
        "    relevant_msgs = [\n",
        "        msg for msg in state[\"messages\"][:-1]  # Exclude current query\n",
        "        if isinstance(msg, (HumanMessage, AIMessage))\n",
        "        and not getattr(msg, \"tool_calls\", None)\n",
        "    ]\n",
        "\n",
        "    if not relevant_msgs:\n",
        "        return {\"conversation_summary\": \"\"}\n",
        "\n",
        "    summary_prompt = \"Summarize the key topics and context from this conversation concisely (2-3 sentences max). Discard irrelevant information, such as misunderstandings or off-topic queries/responses. If there are no key topics, return an empty string:\\n\\n\"\n",
        "    for msg in relevant_msgs[-6:]:  # Last 6 messages for context\n",
        "        role = \"User\" if isinstance(msg, HumanMessage) else \"Assistant\"\n",
        "        summary_prompt += f\"{role}: {msg.content}\\n\"\n",
        "\n",
        "    summary_prompt += \"\\nBrief Summary:\"\n",
        "    summary_response = llm.with_config(temperature=0.2).invoke([SystemMessage(content=summary_prompt)])\n",
        "    return {\"conversation_summary\": summary_response.content}\n",
        "\n",
        "def analyze_and_rewrite_query(state: State):\n",
        "    \"\"\"\n",
        "    Analyzes user query and rewrites it for clarity, optionally using conversation context.\n",
        "    \"\"\"\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    conversation_summary = state.get(\"conversation_summary\", \"\")\n",
        "\n",
        "    context_section = (\n",
        "        f\"**Conversation Context:**\\n{conversation_summary}\"\n",
        "        if conversation_summary.strip()\n",
        "        else \"**Conversation Context:**\\n[First query in conversation]\"\n",
        "    )\n",
        "\n",
        "    # Create analysis prompt\n",
        "    prompt = f\"\"\"Rewrite the user's query to be clear and optimized for information retrieval.\n",
        "                **User Query:**\n",
        "                `\"{last_message.content}\"`\n",
        "\n",
        "                `{context_section}`\n",
        "\n",
        "                **Instructions:**\n",
        "\n",
        "                1.  **If this is a follow-up** (uses pronouns, references previous topics): Resolve all references using the context to make it a self-contained query.\n",
        "                2.  **If it's a new independent query:** Ensure it's already clear and specific.\n",
        "                3.  **If it contains multiple distinct questions:** Split them into a list of separate, focused questions.\n",
        "                4.  **Use specific, keyword-rich language:** Remove vague terms and conversational filler (e.g., \"tell me about\" -> \"facts about\").\n",
        "                5.  **Mark as unclear** if you cannot determine a clear user intent for information retrieval.\n",
        "                    * This includes queries that are **nonsense/gibberish**, **insults**, or statements without an apparent question.\n",
        "                \"\"\"\n",
        "\n",
        "    llm_with_structure = llm.with_config(temperature=0.2).with_structured_output(QueryAnalysis)\n",
        "    response = llm_with_structure.invoke([SystemMessage(content=prompt)])\n",
        "\n",
        "    if response.is_clear:\n",
        "        # Remove all non-system messages\n",
        "        delete_all = [\n",
        "            RemoveMessage(id=m.id)\n",
        "            for m in state[\"messages\"]\n",
        "            if not isinstance(m, SystemMessage)\n",
        "        ]\n",
        "\n",
        "        # Format rewritten query\n",
        "        rewritten = (\n",
        "            \"\\n\".join([f\"{i+1}. {q}\" for i, q in enumerate(response.questions)])\n",
        "            if len(response.questions) > 1\n",
        "            else response.questions[0]\n",
        "        )\n",
        "        return {\n",
        "            \"questionIsClear\": True,\n",
        "            \"messages\": delete_all + [HumanMessage(content=rewritten)]\n",
        "        }\n",
        "    else:\n",
        "        clarification = response.clarification_needed or \"I need more information to understand your question.\"\n",
        "        return {\n",
        "            \"questionIsClear\": False,\n",
        "            \"messages\": [AIMessage(content=clarification)]\n",
        "        }\n",
        "\n",
        "def human_input_node(state: State):\n",
        "    \"\"\"Placeholder node for human-in-the-loop interruption\"\"\"\n",
        "    return {}\n",
        "\n",
        "def route_after_rewrite(state: State) -> Literal[\"agent\", \"human_input\"]:\n",
        "    \"\"\"Route to agent if question is clear, otherwise wait for human input\"\"\"\n",
        "    return \"agent\" if state.get(\"questionIsClear\", False) else \"human_input\"\n",
        "\n",
        "def agent_node(state: State):\n",
        "    \"\"\"Main agent node that processes queries using tools\"\"\"\n",
        "    messages = [SystemMessage(content=agent_system_message.content)] + state[\"messages\"]\n",
        "    response = llm_with_tools.with_config(temperature=0.1).invoke(messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "print(\"âœ“ Graph node functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8c111af",
      "metadata": {
        "id": "d8c111af",
        "outputId": "01d3e221-87f0-49f4-855f-3f64878624ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ“ LangGraph agent compiled\n",
            "  - Nodes: summarize, analyze_rewrite, human_input, agent, tools\n",
            "  - Human-in-the-loop enabled at: human_input\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# BLOCK 10: Graph Construction\n",
        "# ============================================================\n",
        "from langgraph.graph import START, StateGraph\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "# Initialize checkpointer\n",
        "checkpointer = InMemorySaver()\n",
        "\n",
        "# Create graph builder\n",
        "graph_builder = StateGraph(State)\n",
        "\n",
        "# Add nodes\n",
        "graph_builder.add_node(\"summarize\", analyze_chat_and_summarize)\n",
        "graph_builder.add_node(\"analyze_rewrite\", analyze_and_rewrite_query)\n",
        "graph_builder.add_node(\"human_input\", human_input_node)\n",
        "graph_builder.add_node(\"agent\", agent_node)\n",
        "graph_builder.add_node(\"tools\", ToolNode([search_child_chunks, retrieve_parent_chunks]))\n",
        "\n",
        "# Add edges\n",
        "graph_builder.add_edge(START, \"summarize\")\n",
        "graph_builder.add_edge(\"summarize\", \"analyze_rewrite\")\n",
        "graph_builder.add_conditional_edges(\"analyze_rewrite\", route_after_rewrite)\n",
        "graph_builder.add_edge(\"human_input\", \"analyze_rewrite\")\n",
        "graph_builder.add_conditional_edges(\"agent\", tools_condition)\n",
        "graph_builder.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "# Compile graph\n",
        "agent_graph = graph_builder.compile(\n",
        "    checkpointer=checkpointer,\n",
        "    interrupt_before=[\"human_input\"]\n",
        ")\n",
        "\n",
        "print(\"âœ“ LangGraph agent compiled\")\n",
        "print(\"  - Nodes: summarize, analyze_rewrite, human_input, agent, tools\")\n",
        "print(\"  - Human-in-the-loop enabled at: human_input\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5aadf1ed",
      "metadata": {
        "id": "5aadf1ed"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# BLOCK 11: Gradio Interface\n",
        "# ============================================================\n",
        "import gradio as gr\n",
        "import uuid\n",
        "\n",
        "def create_thread_id():\n",
        "    \"\"\"Generate a unique thread ID for each conversation\"\"\"\n",
        "    return {\"configurable\": {\"thread_id\": str(uuid.uuid4())}}\n",
        "\n",
        "def clear_session():\n",
        "    \"\"\"Clear thread for new conversation and clean up checkpointer state\"\"\"\n",
        "    agent_graph.checkpointer.delete_thread(config[\"configurable\"][\"thread_id\"])\n",
        "    config = create_thread_id()\n",
        "\n",
        "def chat_with_agent(message, history):\n",
        "    \"\"\"\n",
        "    Handle chat with human-in-the-loop support.\n",
        "    Returns: response text\n",
        "    \"\"\"\n",
        "    current_state = agent_graph.get_state(config)\n",
        "    if current_state.next:\n",
        "        agent_graph.update_state(config,{\"messages\": [HumanMessage(content=message.strip())]})\n",
        "        result = agent_graph.invoke(None, config)\n",
        "    else:\n",
        "        result = agent_graph.invoke({\"messages\": [HumanMessage(content=message.strip())]}, config)\n",
        "    return result['messages'][-1].content\n",
        "\n",
        "# Initialize thread configuration\n",
        "config = create_thread_id()\n",
        "\n",
        "# Create Gradio interface\n",
        "with gr.Blocks() as demo:\n",
        "    chatbot = gr.Chatbot(height=600, placeholder=\"<strong>Ask me anything!</strong><br><em>I'll search, reason, and act to give you the best answer :)</em>\")\n",
        "    chatbot.clear(clear_session)\n",
        "    gr.ChatInterface(fn=chat_with_agent, type=\"messages\", chatbot=chatbot)\n",
        "\n",
        "print(\"\\nLaunching application...\")\n",
        "demo.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}