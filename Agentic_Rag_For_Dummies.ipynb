{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovanniPasq/agentic-rag-for-dummies/blob/main/Agentic_Rag_For_Dummies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --upgrade langgraph\n",
        "!pip install -qU \"langchain[google-genai]\"\n",
        "!pip install -qU langchain langchain-community langchain-qdrant langchain-huggingface qdrant-client fastembed flashrank langchain-core\n",
        "!pip install --upgrade gradio\n",
        "\n",
        "# Optional: if you want to use Ollama with local models\n",
        "!pip install -qU langchain-ollama"
      ],
      "metadata": {
        "id": "nw5wKiarOk3m"
      },
      "id": "nw5wKiarOk3m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ca49b24",
      "metadata": {
        "id": "4ca49b24"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_qdrant.fastembed_sparse import FastEmbedSparse\n",
        "from qdrant_client import QdrantClient\n",
        "\n",
        "# Configuration\n",
        "DOCS_DIR = \"docs\"  # Directory containing your .md files\n",
        "PARENT_STORE_PATH = \"parent_store\"  # Directory for parent chunk JSON files\n",
        "CHILD_COLLECTION = \"document_child_chunks\"\n",
        "\n",
        "# Create directories if they don't exist\n",
        "os.makedirs(DOCS_DIR, exist_ok=True)\n",
        "os.makedirs(PARENT_STORE_PATH, exist_ok=True)\n",
        "\n",
        "# Initialize LLM (choose one from the previous section)\n",
        "# Example with Ollama:\n",
        "from langchain_ollama import ChatOllama\n",
        "llm = ChatOllama(model=\"qwen3:4b-instruct\", temperature=0)\n",
        "\n",
        "# Or with Google Gemini:\n",
        "# from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "# os.environ[\"GOOGLE_API_KEY\"] = \"your-api-key-here\"\n",
        "# llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-exp\", temperature=0)\n",
        "\n",
        "# Dense embeddings for semantic understanding\n",
        "dense_embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
        ")\n",
        "\n",
        "# Sparse embeddings for keyword matching\n",
        "sparse_embeddings = FastEmbedSparse(\n",
        "    model_name=\"Qdrant/bm25\"\n",
        ")\n",
        "\n",
        "# Qdrant client (local file-based storage)\n",
        "client = QdrantClient(path=\"qdrant_db\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8153a2cf",
      "metadata": {
        "id": "8153a2cf",
        "outputId": "a1e07bd1-2e4a-45ad-b813-d37a66c8ed6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Created collection: document_child_chunks\n"
          ]
        }
      ],
      "source": [
        "from qdrant_client.http import models as qmodels\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain_qdrant.qdrant import RetrievalMode\n",
        "\n",
        "# Get embedding dimension\n",
        "embedding_dimension = len(dense_embeddings.embed_query(\"test\"))\n",
        "\n",
        "def ensure_collection(collection_name):\n",
        "    \"\"\"Create Qdrant collection if it doesn't exist\"\"\"\n",
        "    if not client.collection_exists(collection_name):\n",
        "        client.create_collection(\n",
        "            collection_name=collection_name,\n",
        "            vectors_config=qmodels.VectorParams(\n",
        "                size=embedding_dimension,\n",
        "                distance=qmodels.Distance.COSINE\n",
        "            ),\n",
        "            sparse_vectors_config={\n",
        "                \"sparse\": qmodels.SparseVectorParams()\n",
        "            },\n",
        "        )\n",
        "        print(f\"‚úì Created collection: {collection_name}\")\n",
        "    else:\n",
        "        print(f\"‚úì Collection already exists: {collection_name}\")\n",
        "\n",
        "# Create collection\n",
        "ensure_collection(CHILD_COLLECTION)\n",
        "\n",
        "# Initialize vector store for child chunks\n",
        "child_vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=CHILD_COLLECTION,\n",
        "    embedding=dense_embeddings,\n",
        "    sparse_embedding=sparse_embeddings,\n",
        "    retrieval_mode=RetrievalMode.HYBRID,\n",
        "    sparse_vector_name=\"sparse\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a78ef5db",
      "metadata": {
        "id": "a78ef5db",
        "outputId": "6583dc03-67fc-4807-c3d8-9fdcdf815c2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Starting Hierarchical Indexing\n",
            "==================================================\n",
            "\n",
            "üìÑ Processing: blockchain.md\n",
            "üìÑ Processing: fortinet.md\n",
            "üìÑ Processing: javascript_tutorial.md\n",
            "üìÑ Processing: microservices.md\n",
            "\n",
            "üîç Indexing 2447 child chunks into Qdrant...\n",
            "‚úì Child chunks indexed successfully\n",
            "üíæ Saving 386 parent chunks to JSON...\n",
            "‚úì Parent chunks saved successfully\n",
            "\n",
            "==================================================\n",
            "‚úì Indexing Complete!\n",
            "==================================================\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import glob\n",
        "import json\n",
        "from langchain_text_splitters import (\n",
        "    MarkdownHeaderTextSplitter,\n",
        "    RecursiveCharacterTextSplitter\n",
        ")\n",
        "\n",
        "def index_documents():\n",
        "    \"\"\"Index documents using hierarchical Parent/Child strategy\"\"\"\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"Starting Hierarchical Indexing\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "    # Parent splitter: by Markdown headers\n",
        "    headers_to_split_on = [\n",
        "        (\"#\", \"H1\"),\n",
        "        (\"##\", \"H2\"),\n",
        "        (\"###\", \"H3\")\n",
        "    ]\n",
        "    parent_splitter = MarkdownHeaderTextSplitter(\n",
        "        headers_to_split_on=headers_to_split_on,\n",
        "        strip_headers=False\n",
        "    )\n",
        "\n",
        "    # Child splitter: by character count\n",
        "    child_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=500,\n",
        "        chunk_overlap=100\n",
        "    )\n",
        "\n",
        "    all_child_chunks = []\n",
        "    all_parent_pairs = []\n",
        "\n",
        "    # Check if docs directory has files\n",
        "    md_files = sorted(glob.glob(os.path.join(DOCS_DIR, \"*.md\")))\n",
        "    if not md_files:\n",
        "        print(f\"‚ö†Ô∏è  No .md files found in {DOCS_DIR}/\")\n",
        "        print(\"Please add your Markdown documents to continue.\")\n",
        "        return\n",
        "\n",
        "    # Process each document\n",
        "    for doc_path_str in md_files:\n",
        "        doc_path = Path(doc_path_str)\n",
        "        print(f\"üìÑ Processing: {doc_path.name}\")\n",
        "\n",
        "        try:\n",
        "            with open(doc_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                md_text = f.read()\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error reading {doc_path.name}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Split into parent chunks\n",
        "        parent_chunks = parent_splitter.split_text(md_text)\n",
        "\n",
        "        for i, p_chunk in enumerate(parent_chunks):\n",
        "            # Add metadata\n",
        "            p_chunk.metadata[\"source\"] = str(doc_path)\n",
        "            parent_id = f\"{doc_path.stem}_parent_{i}\"\n",
        "            p_chunk.metadata[\"parent_id\"] = parent_id\n",
        "\n",
        "            # Store parent reference\n",
        "            all_parent_pairs.append((parent_id, p_chunk))\n",
        "\n",
        "            # Split into child chunks\n",
        "            child_chunks = child_splitter.split_documents([p_chunk])\n",
        "            all_child_chunks.extend(child_chunks)\n",
        "\n",
        "    # Save child chunks to Qdrant\n",
        "    if all_child_chunks:\n",
        "        print(f\"\\nüîç Indexing {len(all_child_chunks)} child chunks into Qdrant...\")\n",
        "        try:\n",
        "            child_vector_store.add_documents(all_child_chunks)\n",
        "            print(\"‚úì Child chunks indexed successfully\")\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error indexing child chunks: {e}\")\n",
        "            return\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No child chunks to index\")\n",
        "        return\n",
        "\n",
        "    # Save parent chunks to JSON files\n",
        "    if all_parent_pairs:\n",
        "        print(f\"üíæ Saving {len(all_parent_pairs)} parent chunks to JSON...\")\n",
        "\n",
        "        # Clear existing parent files\n",
        "        for item in os.listdir(PARENT_STORE_PATH):\n",
        "            os.remove(os.path.join(PARENT_STORE_PATH, item))\n",
        "\n",
        "        # Save each parent chunk\n",
        "        for parent_id, doc in all_parent_pairs:\n",
        "            doc_dict = {\n",
        "                \"page_content\": doc.page_content,\n",
        "                \"metadata\": doc.metadata\n",
        "            }\n",
        "            file_path = os.path.join(PARENT_STORE_PATH, f\"{parent_id}.json\")\n",
        "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "                json.dump(doc_dict, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(\"‚úì Parent chunks saved successfully\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"‚úì Indexing Complete!\")\n",
        "    print(\"=\"*50 + \"\\n\")\n",
        "\n",
        "# Run indexing\n",
        "index_documents()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b105aa73",
      "metadata": {
        "id": "b105aa73"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "\n",
        "def search_child_chunks(query: str, k: int = 5) -> List[dict]:\n",
        "    \"\"\"\n",
        "    Search for the top K most relevant child chunks.\n",
        "\n",
        "    Args:\n",
        "        query: Search query string\n",
        "        k: Number of results to return\n",
        "\n",
        "    Returns:\n",
        "        List of dicts with content, parent_id, and source\n",
        "    \"\"\"\n",
        "    try:\n",
        "        results = child_vector_store.similarity_search(query, k=k)\n",
        "        return [\n",
        "            {\n",
        "                \"content\": doc.page_content,\n",
        "                \"parent_id\": doc.metadata.get(\"parent_id\", \"\"),\n",
        "                \"source\": doc.metadata.get(\"source\", \"\")\n",
        "            }\n",
        "            for doc in results\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Error searching child chunks: {e}\")\n",
        "        return []\n",
        "\n",
        "def retrieve_parent_chunks(parent_ids: List[str]) -> List[dict]:\n",
        "    \"\"\"\n",
        "    Retrieve full parent chunks by their IDs.\n",
        "\n",
        "    Args:\n",
        "        parent_ids: List of parent chunk IDs to retrieve\n",
        "\n",
        "    Returns:\n",
        "        List of dicts with content, parent_id, and metadata\n",
        "    \"\"\"\n",
        "    unique_ids = sorted(list(set(parent_ids)))\n",
        "    results = []\n",
        "\n",
        "    for parent_id in unique_ids:\n",
        "        file_path = os.path.join(PARENT_STORE_PATH, f\"{parent_id}.json\")\n",
        "        if os.path.exists(file_path):\n",
        "            try:\n",
        "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                    doc_dict = json.load(f)\n",
        "                    results.append({\n",
        "                        \"content\": doc_dict[\"page_content\"],\n",
        "                        \"parent_id\": parent_id,\n",
        "                        \"metadata\": doc_dict[\"metadata\"]\n",
        "                    })\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading parent chunk {parent_id}: {e}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Bind tools to LLM\n",
        "llm_with_tools = llm.bind_tools([search_child_chunks, retrieve_parent_chunks])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "749f8cc1",
      "metadata": {
        "id": "749f8cc1"
      },
      "outputs": [],
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are an intelligent assistant specialized in answering questions using documents.\n",
        "\n",
        "Follow this precise workflow:\n",
        "\n",
        "**1. Analyze the Question**\n",
        "- Understand what the user is asking\n",
        "- Identify main topics\n",
        "- If complex, split into focused subqueries\n",
        "- Process each subquery through steps 2-7\n",
        "\n",
        "**2. Retrieve Child Chunks**\n",
        "- Use `search_child_chunks` to find relevant small chunks\n",
        "- Choose appropriate K value (default: 5)\n",
        "\n",
        "**3. Evaluate Child Chunks**\n",
        "- Read retrieved content carefully\n",
        "- Determine relevance to the question\n",
        "- Identify `parent_id`s of most relevant chunks\n",
        "- If chunks contain ALL needed information, skip to step 6\n",
        "\n",
        "**4. Assess Need for Context**\n",
        "- If chunks are fragmented, unclear, or incomplete\n",
        "- If they only partially answer the question\n",
        "- Then retrieve parent chunks\n",
        "\n",
        "**5. Retrieve Parent Chunks (if needed)**\n",
        "- Use `retrieve_parent_chunks` with unique `parent_id`s\n",
        "- Read parent chunks for full context\n",
        "\n",
        "**6. Generate Answer**\n",
        "- Base answer exclusively on retrieved information\n",
        "- Combine subquery answers if applicable\n",
        "- Explain concepts clearly\n",
        "- Cite source files (without extension) using metadata\n",
        "- Example: \"This information comes from '[filename]'\"\n",
        "\n",
        "**7. Verify and Iterate**\n",
        "- If initial search found nothing relevant: rephrase query and retry\n",
        "- If parent chunks insufficient: restart from step 1\n",
        "- Maximum 3 attempts per question/subquery\n",
        "- After 3 attempts, ask user to rephrase\n",
        "\n",
        "**Critical Rules:**\n",
        "- Follow steps 1-7 for every question/subquery\n",
        "- Answer only from retrieved chunks\n",
        "- Never fabricate information\n",
        "- Always cite sources\n",
        "\"\"\"\n",
        "\n",
        "system_message = SystemMessage(content=SYSTEM_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f61c778d",
      "metadata": {
        "id": "f61c778d",
        "outputId": "7bbf396a-14bf-4f81-f36f-6dbf658d1672"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úì Agent graph compiled successfully\n"
          ]
        }
      ],
      "source": [
        "from langgraph.graph import MessagesState, START, StateGraph\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "def agent_node(state: MessagesState):\n",
        "    \"\"\"\n",
        "    Agent decision-making node.\n",
        "    Decides which tool to call or generates final response.\n",
        "    \"\"\"\n",
        "    messages = [system_message] + state[\"messages\"]\n",
        "    response = llm_with_tools.invoke(messages)\n",
        "    return {\"messages\": [response]}\n",
        "\n",
        "# Build the execution graph\n",
        "graph_builder = StateGraph(MessagesState)\n",
        "\n",
        "# Add nodes\n",
        "graph_builder.add_node(\"agent\", agent_node)\n",
        "graph_builder.add_node(\n",
        "    \"tools\",\n",
        "    ToolNode([search_child_chunks, retrieve_parent_chunks])\n",
        ")\n",
        "\n",
        "# Define edges\n",
        "graph_builder.add_edge(START, \"agent\")\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tools_condition,  # Routes to tools or END\n",
        ")\n",
        "graph_builder.add_edge(\"tools\", \"agent\")\n",
        "\n",
        "# Compile the graph\n",
        "agent_graph = graph_builder.compile()\n",
        "\n",
        "print(\"‚úì Agent graph compiled successfully\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9141e9c4",
      "metadata": {
        "id": "9141e9c4"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chat_with_agent(message, history):\n",
        "    \"\"\"\n",
        "    Process user message through the agent graph.\n",
        "\n",
        "    Args:\n",
        "        message: User's question\n",
        "        history: Chat history (unused in current implementation)\n",
        "\n",
        "    Returns:\n",
        "        Agent's response as string\n",
        "    \"\"\"\n",
        "    try:\n",
        "        result = agent_graph.invoke({\n",
        "            \"messages\": [HumanMessage(content=message)]\n",
        "        })\n",
        "        return result[\"messages\"][-1].content\n",
        "    except Exception as e:\n",
        "        return f\"‚ùå Error: {str(e)}\\n\\nPlease try rephrasing your question.\"\n",
        "\n",
        "# Create and launch interface\n",
        "demo = gr.ChatInterface(\n",
        "    fn=chat_with_agent,\n",
        "    title=\"ü§ñ Agentic RAG Assistant\",\n",
        "    description=\"Ask questions about your documents. The agent will intelligently retrieve and combine information.\"\n",
        ")\n",
        "\n",
        "print(\"\\nüöÄ Launching chat interface...\")\n",
        "demo.launch(share=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}