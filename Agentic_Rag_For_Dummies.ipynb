{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOKkyDygL0yt35z7UNnuoh6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GiovanniPasq/agentic-rag-for-dummies/blob/main/Agentic_Rag_For_Dummies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --upgrade langgraph\n",
        "!pip install -qU \"langchain[google-genai]\"\n",
        "!pip install -qU langchain langchain-community langchain-qdrant langchain-huggingface qdrant-client fastembed flashrank langchain-core\n",
        "!pip install --upgrade gradio\n",
        "\n",
        "# Optional: if you want to use Ollama with local models\n",
        "!pip install -qU langchain-ollama"
      ],
      "metadata": {
        "id": "JD7B9iRjFuvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dO6vDNH_k4u"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# Set your Google API key\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your-api-key-here\"\n",
        "\n",
        "# Initialize the LLM with zero temperature for consistent outputs\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-2.0-flash-001\",\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "\"\"\"\n",
        "from langchain_ollama.chat_models import ChatOllama\n",
        "\n",
        "llm = ChatOllama(\n",
        "    model=\"your-model\",\n",
        "    temperature=0\n",
        ")\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models as qmodels\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_qdrant.fastembed_sparse import FastEmbedSparse\n",
        "from langchain_qdrant import QdrantVectorStore\n",
        "from langchain_qdrant.qdrant import RetrievalMode\n",
        "\n",
        "# Configuration\n",
        "DOCUMENT_DIR = \"docs\"\n",
        "SUMMARY_DIR = \"summaries\"\n",
        "DB_PATH = \"qdrant_db\"\n",
        "SUMMARY_COLLECTION = \"document_summaries\"\n",
        "\n",
        "# Initialize embeddings\n",
        "dense_embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"intfloat/multilingual-e5-large\"\n",
        ")\n",
        "sparse_embeddings = FastEmbedSparse(\n",
        "    model_name=\"Qdrant/bm25\"\n",
        ")\n",
        "\n",
        "# Create Qdrant client\n",
        "client = QdrantClient(path=DB_PATH)\n",
        "embedding_dimension = len(dense_embeddings.embed_query(\"test\"))\n",
        "\n",
        "def ensure_collection(collection_name):\n",
        "    \"\"\"Create collection if it doesn't exist\"\"\"\n",
        "    if not client.collection_exists(collection_name):\n",
        "        client.create_collection(\n",
        "            collection_name=collection_name,\n",
        "            vectors_config=qmodels.VectorParams(\n",
        "                size=embedding_dimension,\n",
        "                distance=qmodels.Distance.COSINE\n",
        "            ),\n",
        "            sparse_vectors_config={\n",
        "                \"sparse\": qmodels.SparseVectorParams()\n",
        "            },\n",
        "        )\n",
        "\n",
        "# Create collections\n",
        "ensure_collection(SUMMARY_COLLECTION)\n",
        "\n",
        "# Initialize vector stores\n",
        "summary_vector_store = QdrantVectorStore(\n",
        "    client=client,\n",
        "    collection_name=SUMMARY_COLLECTION,\n",
        "    embedding=dense_embeddings,\n",
        "    sparse_embedding=sparse_embeddings,\n",
        "    retrieval_mode=RetrievalMode.HYBRID,\n",
        "    sparse_vector_name=\"sparse\"\n",
        ")"
      ],
      "metadata": {
        "id": "yqkDzEqEHiyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import re\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "summary_documents = []\n",
        "\n",
        "# Load all summary files\n",
        "for file_path in sorted(glob.glob(os.path.join(SUMMARY_DIR, \"*_summary.md\"))):\n",
        "    base_name = os.path.basename(file_path)\n",
        "    # Extract document ID from filename\n",
        "    document_id = re.sub(r\"_summary\\.md$\", \"\", base_name, flags=re.I).lower()\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        content = f.read()\n",
        "\n",
        "    summary_documents.append(\n",
        "        Document(\n",
        "            page_content=content,\n",
        "            metadata={\"document_id\": document_id, \"source\": base_name}\n",
        "        )\n",
        "    )\n",
        "\n",
        "# Index summaries in vector database\n",
        "_ = summary_vector_store.add_documents(summary_documents)"
      ],
      "metadata": {
        "id": "cKa1NXcQNqOC"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from pathlib import Path\n",
        "\n",
        "def search_summaries(query: str, k: int = 3) -> List[dict]:\n",
        "    \"\"\"\n",
        "    Search for the top K most relevant document summaries.\n",
        "\n",
        "    Args:\n",
        "        query: The search query\n",
        "        k: Number of results to return\n",
        "\n",
        "    Returns:\n",
        "        List of relevant summary documents with their metadata\n",
        "    \"\"\"\n",
        "    results = summary_vector_store.similarity_search(query, k=k)\n",
        "    # Convert to dict format that can be passed between tools\n",
        "    return [\n",
        "        {\n",
        "            \"content\": doc.page_content,\n",
        "            \"document_id\": doc.metadata.get(\"document_id\", \"\"),\n",
        "            \"source\": doc.metadata.get(\"source\", \"\")\n",
        "        }\n",
        "        for doc in results\n",
        "    ]\n",
        "\n",
        "def retrieve_full_documents(document_ids: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Retrieve complete documents based on document IDs.\n",
        "\n",
        "    Args:\n",
        "        document_ids: List of document IDs to retrieve\n",
        "\n",
        "    Returns:\n",
        "        List of full document contents\n",
        "    \"\"\"\n",
        "    full_documents = []\n",
        "\n",
        "    for doc_id in document_ids:\n",
        "        if not doc_id:\n",
        "            continue\n",
        "\n",
        "        # Construct path to full document\n",
        "        document_path = Path(DOCUMENT_DIR) / f\"{doc_id}\"\n",
        "        print(document_path)\n",
        "        if document_path.exists():\n",
        "            with open(document_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "                full_documents.append(content)\n",
        "\n",
        "    return full_documents\n",
        "\n",
        "# Bind tools to LLM\n",
        "llm_with_tools = llm.bind_tools([search_summaries, retrieve_full_documents])"
      ],
      "metadata": {
        "id": "7NyOI9DDM2oo"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "SYSTEM_PROMPT = \"\"\"You are an intelligent document retrieval assistant specialized in answering questions accurately using available documents.\n",
        "\n",
        "Your task follows this precise workflow:\n",
        "\n",
        "1. **Analyze the question**:\n",
        "   - Understand what the user is asking\n",
        "   - Identify the main topic and any sub-topics\n",
        "\n",
        "2. **Rewrite and split if necessary**:\n",
        "   - Rephrase the question if it's unclear\n",
        "   - If the question covers multiple different topics, split it into sub-queries\n",
        "   - Each sub-query should address a single, specific topic\n",
        "\n",
        "3. **Retrieve top X summary documents**:\n",
        "   - Decide how many summary documents to retrieve (the X value is your choice based on query complexity)\n",
        "   - Use the search_summaries tool for each sub-query\n",
        "   - Evaluate each retrieved summary to determine if it's relevant to the question\n",
        "   - Discard irrelevant summaries\n",
        "\n",
        "4. **Return exact document names**:\n",
        "   - From the relevant summaries, extract the exact document_id with extension\n",
        "   - List which documents you're going to retrieve\n",
        "\n",
        "5. **Retrieve complete documents and provide answer**:\n",
        "   - Use the retrieve_full_documents tool with the document_ids\n",
        "   - Read the full documents to find the answer\n",
        "\n",
        "6. **Verify document relevance**:\n",
        "   - Check if each complete document is actually pertinent to the question\n",
        "   - Discard documents that are not relevant\n",
        "   - **If NONE of the documents are relevant, GO BACK TO STEP 1 and try again with different search terms**\n",
        "\n",
        "7. **Provide clear and detailed answer**:\n",
        "   - Give a comprehensive response based on the documents\n",
        "   - Explain concepts clearly, assuming the user has no prior knowledge of the topic\n",
        "   - Use simple language and avoid jargon when possible\n",
        "\n",
        "8. **Verify answer completeness**:\n",
        "   - Check that your complete answer is relevant and fully addresses the question\n",
        "   - Ensure all sub-queries (if any) have been answered\n",
        "\n",
        "9. **If answer is not satisfactory**:\n",
        "   - **GO BACK TO STEP 1** and start the process again with a different approach\n",
        "\n",
        "10. **Loop limit**:\n",
        "    - **Repeat this entire loop a MAXIMUM of 3 times**\n",
        "    - After 3 complete attempts, if you're still not confident in your answer, politely ask the user to rephrase their question more clearly\n",
        "\n",
        "**Critical rules**:\n",
        "- You MUST follow steps 1-10 in order\n",
        "- You MUST go back to step 1 if documents are not relevant (step 6) or answer is not satisfactory (step 9)\n",
        "- You MUST NOT exceed 3 complete loops through the entire process\n",
        "- Always base your answers strictly on the retrieved documents\n",
        "- Never make up information that isn't in the documents\n",
        "\"\"\"\n",
        "\n",
        "system_message = SystemMessage(content=SYSTEM_PROMPT)"
      ],
      "metadata": {
        "id": "sdNtaQ-9au91"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "from langgraph.graph import MessagesState, START, StateGraph\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "# Define the agent's decision-making node\n",
        "def agent_node(state: MessagesState):\n",
        "    \"\"\"Agent decides which tool to call or generates final response\"\"\"\n",
        "    return {\n",
        "        \"messages\": llm_with_tools.invoke(\n",
        "            [system_message] + state[\"messages\"]\n",
        "        )\n",
        "    }\n",
        "\n",
        "\n",
        "# Build the graph\n",
        "graph_builder = StateGraph(MessagesState)\n",
        "\n",
        "# Add nodes\n",
        "graph_builder.add_node(\"agent\", agent_node)\n",
        "graph_builder.add_node(\"tools\", ToolNode([search_summaries, retrieve_full_documents]))\n",
        "\n",
        "# Define edges\n",
        "graph_builder.add_edge(START, \"agent\")\n",
        "graph_builder.add_conditional_edges(\n",
        "    \"agent\",\n",
        "    tools_condition,  # Decides if tools are needed or if we should end\n",
        ")\n",
        "graph_builder.add_edge(\"tools\", \"agent\")  # After tool use, return to agent\n",
        "\n",
        "# Compile the graph\n",
        "agent_graph = graph_builder.compile()\n",
        "\n",
        "# Visualize the graph (optional)\n",
        "from IPython.display import Image, display\n",
        "display(Image(agent_graph.get_graph(xray=True).draw_mermaid_png()))"
      ],
      "metadata": {
        "id": "JUXxUFLMOJPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def chat_with_agent(message, history):\n",
        "    \"\"\"Process user message and return agent's response\"\"\"\n",
        "    result = agent_graph.invoke({\n",
        "        \"messages\": [HumanMessage(content=message)]\n",
        "    })\n",
        "    print(result)\n",
        "    return result[\"messages\"][-1].content\n",
        "\n",
        "# Launch Gradio interface\n",
        "demo = gr.ChatInterface(fn=chat_with_agent)\n",
        "demo.launch(share=False)"
      ],
      "metadata": {
        "id": "TYkEfMFiOMBv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}